{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c02edf",
   "metadata": {},
   "source": [
    "1) Write a python program to display all the header tags from wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95f2041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72fa8ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9062293c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\prachi\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\prachi\\anaconda3\\lib\\site-packages (from bs4) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\prachi\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\prachi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\prachi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\prachi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\prachi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\prachi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\prachi\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57deec35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\prachi\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prachi\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\prachi\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\prachi\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prachi\\anaconda3\\lib\\site-packages (from requests) (3.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\prachi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\prachi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\prachi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\prachi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\prachi\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\prachi\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8160fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60821239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2db4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1019f939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html=requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7551ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2a43cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all the header tags :\n",
      "\n",
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\">Main Page</h1>\n",
      "\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n",
      "\n",
      "<h2>Navigation menu</h2>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-personal-label\">\n",
      "<span class=\"vector-menu-heading-label\">Personal tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-namespaces-label\">\n",
      "<span class=\"vector-menu-heading-label\">Namespaces</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-views-label\">\n",
      "<span class=\"vector-menu-heading-label\">Views</span>\n",
      "</h3>\n",
      "\n",
      "<h3>\n",
      "<label for=\"searchInput\">Search</label>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-navigation-label\">\n",
      "<span class=\"vector-menu-heading-label\">Navigation</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-interaction-label\">\n",
      "<span class=\"vector-menu-heading-label\">Contribute</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-tb-label\">\n",
      "<span class=\"vector-menu-heading-label\">Tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-coll-print_export-label\">\n",
      "<span class=\"vector-menu-heading-label\">Print/export</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-wikibase-otherprojects-label\">\n",
      "<span class=\"vector-menu-heading-label\">In other projects</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-lang-label\">\n",
      "<span class=\"vector-menu-heading-label\">Languages</span>\n",
      "</h3>\n"
     ]
    }
   ],
   "source": [
    "bs = BeautifulSoup(html.text, 'lxml')\n",
    "titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "print('List all the header tags :', *titles, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388bb6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d64eb5c9",
   "metadata": {},
   "source": [
    "2) Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release)\n",
    "and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71060032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shawshank Redemption - (1994) - 9.3\n",
      "The Godfather - (1972) - 9.2\n",
      "The Dark Knight - (2008) - 9.0\n",
      "The Lord of the Rings: The Return of the King - (2003) - 9.0\n",
      "Schindler's List - (1993) - 9.0\n",
      "The Godfather Part II - (1974) - 9.0\n",
      "12 Angry Men - (1957) - 9.0\n",
      "Pulp Fiction - (1994) - 8.9\n",
      "Inception - (2010) - 8.8\n",
      "The Lord of the Rings: The Two Towers - (2002) - 8.8\n",
      "Fight Club - (1999) - 8.8\n",
      "The Lord of the Rings: The Fellowship of the Ring - (2001) - 8.8\n",
      "Forrest Gump - (1994) - 8.8\n",
      "Il buono, il brutto, il cattivo - (1966) - 8.8\n",
      "The Matrix - (1999) - 8.7\n",
      "Goodfellas - (1990) - 8.7\n",
      "The Empire Strikes Back - (1980) - 8.7\n",
      "One Flew Over the Cuckoo's Nest - (1975) - 8.7\n",
      "Interstellar - (2014) - 8.6\n",
      "Cidade de Deus - (2002) - 8.6\n",
      "Sen to Chihiro no kamikakushi - (2001) - 8.6\n",
      "Saving Private Ryan - (1998) - 8.6\n",
      "The Green Mile - (1999) - 8.6\n",
      "La vita è bella - (1997) - 8.6\n",
      "Se7en - (1995) - 8.6\n",
      "Terminator 2: Judgment Day - (1991) - 8.6\n",
      "The Silence of the Lambs - (1991) - 8.6\n",
      "Star Wars - (1977) - 8.6\n",
      "Seppuku - (1962) - 8.6\n",
      "Shichinin no samurai - (1954) - 8.6\n",
      "It's a Wonderful Life - (1946) - 8.6\n",
      "Gisaengchung - (2019) - 8.5\n",
      "Whiplash - (2014) - 8.5\n",
      "Top Gun: Maverick - (2022) - 8.5\n",
      "The Intouchables - (2011) - 8.5\n",
      "The Prestige - (2006) - 8.5\n",
      "The Departed - (2006) - 8.5\n",
      "The Pianist - (2002) - 8.5\n",
      "Gladiator - (2000) - 8.5\n",
      "American History X - (1998) - 8.5\n",
      "The Usual Suspects - (1995) - 8.5\n",
      "Léon - (1994) - 8.5\n",
      "The Lion King - (1994) - 8.5\n",
      "Nuovo Cinema Paradiso - (1988) - 8.5\n",
      "Hotaru no haka - (1988) - 8.5\n",
      "Back to the Future - (1985) - 8.5\n",
      "Apocalypse Now - (1979) - 8.5\n",
      "Alien - (1979) - 8.5\n",
      "Once Upon a Time in the West - (1968) - 8.5\n",
      "Psycho - (1960) - 8.5\n",
      "Rear Window - (1954) - 8.5\n",
      "Casablanca - (1942) - 8.5\n",
      "Modern Times - (1936) - 8.5\n",
      "City Lights - (1931) - 8.5\n",
      "Capharnaüm - (2018) - 8.4\n",
      "Joker - (I 2019) - 8.4\n",
      "Kimi no na wa. - (2016) - 8.4\n",
      "Spider-Man: Into the Spider-Verse - (2018) - 8.4\n",
      "Avengers: Endgame - (2019) - 8.4\n",
      "Avengers: Infinity War - (2018) - 8.4\n",
      "Coco - (I 2017) - 8.4\n",
      "Django Unchained - (2012) - 8.4\n",
      "The Dark Knight Rises - (2012) - 8.4\n",
      "3 Idiots - (2009) - 8.4\n",
      "WALL·E - (2008) - 8.4\n",
      "The Lives of Others - (2006) - 8.4\n",
      "Oldeuboi - (2003) - 8.4\n",
      "Memento - (2000) - 8.4\n",
      "American Beauty - (1999) - 8.4\n",
      "Mononoke-hime - (1997) - 8.4\n",
      "Braveheart - (1995) - 8.4\n",
      "Idi i smotri - (1985) - 8.4\n",
      "Aliens - (1986) - 8.4\n",
      "Amadeus - (1984) - 8.4\n",
      "Raiders of the Lost Ark - (1981) - 8.4\n",
      "Das Boot - (1981) - 8.4\n",
      "The Shining - (1980) - 8.4\n",
      "Tengoku to jigoku - (1963) - 8.4\n",
      "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb - (1964) - 8.4\n",
      "Witness for the Prosecution - (1957) - 8.4\n",
      "Paths of Glory - (1957) - 8.4\n",
      "Sunset Blvd. - (1950) - 8.4\n",
      "The Great Dictator - (1940) - 8.4\n",
      "Jagten - (2012) - 8.3\n",
      "Toy Story 3 - (2010) - 8.3\n",
      "Inglourious Basterds - (2009) - 8.3\n",
      "Eternal Sunshine of the Spotless Mind - (2004) - 8.3\n",
      "Requiem for a Dream - (2000) - 8.3\n",
      "Good Will Hunting - (1997) - 8.3\n",
      "Toy Story - (1995) - 8.3\n",
      "Reservoir Dogs - (1992) - 8.3\n",
      "Once Upon a Time in America - (1984) - 8.3\n",
      "Star Wars: Episode VI - Return of the Jedi - (1983) - 8.3\n",
      "2001: A Space Odyssey - (1968) - 8.3\n",
      "Lawrence of Arabia - (1962) - 8.3\n",
      "North by Northwest - (1959) - 8.3\n",
      "Vertigo - (1958) - 8.3\n",
      "Singin' in the Rain - (1952) - 8.3\n",
      "Citizen Kane - (1941) - 8.3\n",
      "M - Eine Stadt sucht einen Mörder - (1931) - 8.3\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url = 'https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc'\n",
    "#url2 = 'https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&start=51&ref_=adv_nxt'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "movies = soup.select('div.lister-item-content h3.lister-item-header a')\n",
    "ratings = soup.select('div.lister-item-content div.ratings-bar div.ratings-imdb-rating strong')\n",
    "years = soup.select('div.lister-item-content h3.lister-item-header span.lister-item-year')\n",
    "\n",
    "imdb = []\n",
    "\n",
    "for index in range(0,len(years)):\n",
    "    data = {\n",
    "        \"Movie\":movies[index].get_text(),\n",
    "        \"Rating\":ratings[index].get_text(),\n",
    "        \"Year\":years[index].get_text().replace(\")\", \"\").replace(\"(\", \"\")\n",
    "    }\n",
    "    imdb.append(data)\n",
    "\n",
    "url = 'https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc&start=51&ref_=adv_nxt'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "movies = soup.select('div.lister-item-content h3.lister-item-header a')\n",
    "ratings = soup.select('div.lister-item-content div.ratings-bar div.ratings-imdb-rating strong')\n",
    "years = soup.select('div.lister-item-content h3.lister-item-header span.lister-item-year')\n",
    "\n",
    "for index in range(0,len(years)):\n",
    "    data = {\n",
    "        \"Movie\":movies[index].get_text(),\n",
    "        \"Rating\":ratings[index].get_text(),\n",
    "        \"Year\":years[index].get_text().replace(\")\", \"\").replace(\"(\", \"\")\n",
    "    }\n",
    "    imdb.append(data)\n",
    "\n",
    "    \n",
    "for movie in imdb:\n",
    "    print(movie['Movie'],'-','('+movie['Year']+')','-',movie['Rating'])\n",
    "\n",
    "    \n",
    "df=pd.DataFrame(imdb)\n",
    "df.to_csv('imdb_top_100_movies.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27c81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "771bcdd0",
   "metadata": {},
   "source": [
    "3) Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of\n",
    "release) and make data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea80d5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ship of Theseus - (2012) - 8\n",
      "Iruvar - (1997) - 0\n",
      "Kaagaz Ke Phool - (1959) - Rate\n",
      "Lagaan: Once Upon a Time in India - (2001) - 1\n",
      "Pather Panchali - (1955) - Rate\n",
      "Charulata - (1964) - 2\n",
      "Rang De Basanti - (2006) - Rate\n",
      "Dev.D - (2009) - 3\n",
      "3 Idiots - (2009) - Rate\n",
      "Awaara - (1951) - 4\n",
      "Nayakan - (1987) - Rate\n",
      "Aparajito - (1956) - 5\n",
      "Pushpaka Vimana - (1987) - Rate\n",
      "Pyaasa - (1957) - 6\n",
      "Ghatashraddha - (1977) - Rate\n",
      "Sholay - (1975) - 7\n",
      "Aradhana - (1969) - Rate\n",
      "Do Ankhen Barah Haath - (1957) - 8\n",
      "Bombay - (1995) - Rate\n",
      "Neecha Nagar - (1946) - 9\n",
      "Do Bigha Zamin - (1953) - Rate\n",
      "Garm Hava - (1974) - 10\n",
      "Piravi - (1989) - Rate\n",
      "Mughal-E-Azam - (1960) - 8.4\n",
      "Amma Ariyan - (1986) - 0\n",
      "Madhumati - (1958) - Rate\n",
      "Goopy Gyne Bagha Byne - (1969) - 1\n",
      "Gangs of Wasseypur - (2012) - Rate\n",
      "Guide - (1965) - 2\n",
      "Satya - (1998) - Rate\n",
      "Roja - (1992) - 3\n",
      "Mr. India - (1987) - Rate\n",
      "The Cloud-Capped Star - (1960) - 4\n",
      "Harishchandrachi Factory - (2009) - Rate\n",
      "Masoom - (1983) - 5\n",
      "Agneepath - (1990) - Rate\n",
      "Tabarana Kathe - (1986) - 6\n",
      "Zakhm - (1998) - Rate\n",
      "Dil Chahta Hai - (2001) - 7\n",
      "Bhaag Milkha Bhaag - (2013) - Rate\n",
      "Chupke Chupke - (1975) - 8\n",
      "Dilwale Dulhania Le Jayenge - (1995) - Rate\n",
      "Taare Zameen Par - (2007) - 9\n",
      "Ardh Satya - (1983) - Rate\n",
      "Bhumika - (1977) - 10\n",
      "Enthiran - (2010) - Rate\n",
      "Sadma - (1983) - 7.8\n",
      "Shwaas - (2004) - 0\n",
      "Lamhe - (1991) - Rate\n",
      "Haqeeqat - (1964) - 1\n",
      "Shree 420 - (1955) - Rate\n",
      "Kannathil Muthamittal - (2002) - 2\n",
      "Hum Aapke Hain Koun..! - (1994) - Rate\n",
      "Ustad Hotel - (2012) - 3\n",
      "Bandit Queen - (1994) - Rate\n",
      "Lakshya - (2004) - 4\n",
      "Black Friday - (2004) - Rate\n",
      "Manthan - (1976) - 5\n",
      "Apoorva Raagangal - (1975) - Rate\n",
      "English Vinglish - (2012) - 6\n",
      "Jewel Thief - (1967) - Rate\n",
      "Pakeezah - (1972) - 7\n",
      "Maqbool - (2003) - Rate\n",
      "Jis Desh Men Ganga Behti Hai - (1960) - 8\n",
      "Sahib Bibi Aur Ghulam - (1962) - Rate\n",
      "Shatranj Ke Khilari - (1977) - 9\n",
      "Narthanasala - (1963) - Rate\n",
      "Chandni Bar - (2001) - 10\n",
      "Vaaranam Aayiram - (2008) - Rate\n",
      "Mr. and Mrs. Iyer - (2002) - 8.1\n",
      "Chandni - (1989) - 0\n",
      "English, August - (1994) - Rate\n",
      "Celluloid - (2013) - 1\n",
      "Sagara Sangamam - (1983) - Rate\n",
      "Munna Bhai M.B.B.S. - (2003) - 2\n",
      "Saaransh - (1984) - Rate\n",
      "Guddi - (1971) - 3\n",
      "Vanaja - (2006) - Rate\n",
      "Vazhakku Enn 18/9 - (2012) - 4\n",
      "Gangaajal - (2003) - Rate\n",
      "Angoor - (1982) - 5\n",
      "Guru - (2007) - Rate\n",
      "Andaz Apna Apna - (1994) - 6\n",
      "Sangam - (I 1964) - Rate\n",
      "Oka Oori Katha - (1978) - 7\n",
      "Bhuvan Shome - (1969) - Rate\n",
      "Border - (I 1997) - 8\n",
      "Parineeta - (2005) - Rate\n",
      "Devdas - (1955) - 9\n",
      "Abohomaan - (2009) - Rate\n",
      "Kuch Kuch Hota Hai - (1998) - 10\n",
      "Pithamagan - (2003) - Rate\n",
      "Veyyil - (2006) - 8.3\n",
      "Chemmeen - (1965) - 0\n",
      "Jaane Bhi Do Yaaro - (1983) - Rate\n",
      "Apur Sansar - (1959) - 1\n",
      "Kanchivaram - (2008) - Rate\n",
      "Monsoon Wedding - (2001) - 2\n",
      "Black - (2005) - Rate\n",
      "Deewaar - (1975) - 3\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url='https://www.imdb.com/list/ls056092300/'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "movies=soup.select('div.lister-item-content h3.lister-item-header a')\n",
    "ratings=soup.select('div.ipl-rating-widget div.ipl-rating-star span.ipl-rating-star__rating')\n",
    "years=soup.select('div.lister-item-content h3.lister-item-header span.lister-item-year')\n",
    "\n",
    "imdb=[]\n",
    "\n",
    "for index in range(0,len(years)): \n",
    "    data = {\n",
    "        \"Movie\":movies[index].get_text(),\n",
    "        \"Rating\":ratings[index].get_text(),\n",
    "        \"Year\":years[index].get_text().replace(\")\", \"\").replace(\"(\", \"\")\n",
    "    }\n",
    "    imdb.append(data)\n",
    "    \n",
    "for movie in imdb:\n",
    "    print(movie['Movie'],'-','('+movie['Year']+')','-',movie['Rating'])\n",
    "\n",
    "    \n",
    "df=pd.DataFrame(imdb)\n",
    "df.to_csv('imdb_top_Indian_100_movies.csv',index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba3298e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9890abba",
   "metadata": {},
   "source": [
    "4) Write s python program to display list of respected former presidents of India(i.e. Name , Term of office) \n",
    "from https://presidentofindia.nic.in/former-presidents.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5bf43b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shri Ram Nath Kovind (birth - 1945) \n",
      " (Term of Office: 25 July, 2017 to 25 July, 2022 )\n",
      "Shri Pranab Mukherjee (1935-2020) \n",
      " (https://ramnathkovind.nic.in)\n",
      "Smt Pratibha Devisingh Patil (birth - 1934) \n",
      " (Term of Office: 25 July, 2012 to 25 July, 2017 )\n",
      "DR. A.P.J. Abdul Kalam (1931-2015) \n",
      " (http://pranabmukherjee.nic.in)\n",
      "Shri K. R. Narayanan (1920 - 2005) \n",
      " (Term of Office: 25 July, 2007 to 25 July, 2012 )\n",
      "Dr Shankar Dayal Sharma (1918-1999) \n",
      " (http://pratibhapatil.nic.in)\n",
      "Shri R Venkataraman (1910-2009) \n",
      " (Term of Office: 25 July, 2002 to 25 July, 2007 )\n",
      "Giani Zail Singh (1916-1994) \n",
      " (http://abdulkalam.nic.in)\n",
      "Shri Neelam Sanjiva Reddy (1913-1996) \n",
      " (Term of Office: 25 July, 1997 to 25 July, 2002 )\n",
      "Dr. Fakhruddin Ali Ahmed (1905-1977) \n",
      " (Term of Office: 25 July, 1992 to 25 July, 1997 )\n",
      "Shri Varahagiri Venkata Giri (1894-1980) \n",
      " (Term of Office: 25 July, 1987 to 25 July, 1992 )\n",
      "Dr. Zakir Husain (1897-1969) \n",
      " (Term of Office: 25 July, 1982 to 25 July, 1987 )\n",
      "Dr. Sarvepalli Radhakrishnan (1888-1975) \n",
      " (Term of Office: 25 July, 1977 to 25 July, 1982 )\n",
      "Dr. Rajendra Prasad (1884-1963)  \n",
      " (Term of Office: 24 August, 1974 to 11 February, 1977)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url='https://presidentofindia.nic.in/former-presidents.htm'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "names=soup.select('div.presidentListing h3')\n",
    "office=soup.select('div.presidentListing p')\n",
    "\n",
    "list=[]\n",
    "\n",
    "for index in range(0,len(names)): \n",
    "    data = {\n",
    "        \"Name\":names[index].get_text(),\n",
    "        \"TermOfOffice\":office[index].get_text().replace(\"(\",\"\").replace(\")\",\"\")        \n",
    "    }\n",
    "    list.append(data)\n",
    "    \n",
    "for name in list:\n",
    "    print(name['Name'],'\\n','('+name['TermOfOffice']+')')\n",
    "\n",
    "    \n",
    "df=pd.DataFrame(list)\n",
    "df.to_csv('presidents_of_India.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881911ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9460b1a2",
   "metadata": {},
   "source": [
    "5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a38c4c",
   "metadata": {},
   "source": [
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "83322996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Zealand (19) 2,355 124\n",
      "India (31) 3,447 111\n",
      "Pakistan (22) 2,354 107\n",
      "Australia (23) 2,325 101\n",
      "South Africa (21) 2,111 101\n",
      "Bangladesh (30) 2,753 92\n",
      "Sri Lanka (29) 2,658 92\n",
      "West Indies (41) 2,902 71\n",
      "Afghanistan (18) 1,238 69\n",
      "Ireland (23) 1,214 53\n",
      "Scotland (27) 1,254 46\n",
      "Zimbabwe (23) 910 40\n",
      "Netherlands (21) 673 32\n",
      "UAE (22) 697 32\n",
      "United States (23) 725 32\n",
      "Oman (30) 919 31\n",
      "Namibia (15) 369 25\n",
      "Nepal (22) 331 15\n",
      "Papua New Guinea (22) 134 6\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url='https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "teams=soup.select('td.rankings-block__banner--team-name span.u-hide-phablet')\n",
    "matches=soup.select('tr.rankings-block__banner td.rankings-block__banner--matches')\n",
    "points=soup.select('td.rankings-block__banner--points')\n",
    "ratings=soup.select('td.rankings-block__banner--rating.u-text-right')\n",
    "\n",
    "list=[]\n",
    "\n",
    "for index in range(0,len(teams)): \n",
    "    data = {\n",
    "        \"Team\":teams[index].get_text(),\n",
    "        \"Match\":matches[index].get_text(),\n",
    "        \"Points\":points[index].get_text(),\n",
    "        \"Rating\":ratings[index].get_text().strip()\n",
    "    }\n",
    "    list.append(data)\n",
    "\n",
    "url='https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "teams=soup.select('td.table-body__cell.rankings-table__team span.u-hide-phablet')\n",
    "matches=soup.select('td.table-body__cell.u-center-text')[::2]\n",
    "points=soup.select('td.table-body__cell.u-center-text')[1::2]\n",
    "ratings=soup.select('td.table-body__cell.u-text-right.rating')\n",
    "\n",
    "\n",
    "for index in range(1,len(teams)): \n",
    "    data = {\n",
    "        \"Team\":teams[index].get_text(),\n",
    "        \"Match\":matches[index].get_text(),\n",
    "        \"Points\":points[index].get_text(),\n",
    "        \"Rating\":ratings[index].get_text()\n",
    "    }\n",
    "    list.append(data)\n",
    "    \n",
    "    \n",
    "for name in list:\n",
    "    print(name['Team'],'('+name['Match']+')',name['Points'],name['Rating'])\n",
    "\n",
    "df=pd.DataFrame(list)\n",
    "df.to_csv('10ODI_men_Teams.csv',index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e8bee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ff21c75",
   "metadata": {},
   "source": [
    "b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44d13032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (Babar Azam) \n",
      "2 (Rassie van der Dussen) SA\n",
      "3 (Quinton de Kock) SA\n",
      "4 (Imam-ul-Haq) PAK\n",
      "5 (Virat Kohli) IND\n",
      "6 (Rohit Sharma) IND\n",
      "7 (David Warner) AUS\n",
      "8 (Jonny Bairstow) ENG\n",
      "9 (Ross Taylor) NZ\n",
      "10 (Aaron Finch) AUS\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url='https://www.icc-cricket.com/rankings/mens/player-rankings/odi'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "# Find batting Section\n",
    "records=soup.find(\"div\", {\"data-cricket-role\" : \"batting\"})\n",
    "\n",
    "# For ranking 1st only\n",
    "ranking = records.find_all(\"span\",{\"class\" : \"rankings-block__pos-number\"})\n",
    "names = records.find_all(\"div\",{\"class\" : \"rankings-block__banner--name\"})\n",
    "country = records.find_all(\"div\",{\"class\" : \"flag-15\"})\n",
    "\n",
    "list=[]\n",
    "\n",
    "for index in range(0,len(names)): \n",
    "    data = {\n",
    "        \"Ranking\":ranking[0].get_text().strip(),\n",
    "        \"Name\":names[0].get_text().strip(),\n",
    "        \"Country\":country[0].get_text().strip()\n",
    "    }\n",
    "    list.append(data)\n",
    "\n",
    "\n",
    "#For ranking from 2nd to 10th\n",
    "url='https://www.icc-cricket.com/rankings/mens/player-rankings/odi'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "# Find batting Section\n",
    "records=soup.find(\"div\", {\"data-cricket-role\" : \"batting\"})\n",
    "\n",
    "rankings = records.find_all(\"span\",{\"class\" : \"rankings-table__pos-number\"})\n",
    "names = records.find_all(\"td\",{\"class\" : \"table-body__cell name\"})\n",
    "countries = records.find_all(\"span\",{\"class\" : \"table-body__logo-text\"})\n",
    "\n",
    "for index in range(0,len(names)): \n",
    "    data = {\n",
    "        \"Ranking\":rankings[index].get_text().strip(),\n",
    "        \"Name\":names[index].get_text().strip(),\n",
    "        \"Country\":countries[index].get_text().strip()    \n",
    "    }\n",
    "    list.append(data)\n",
    "    \n",
    "for name in list:\n",
    "    print(name['Ranking'],'('+name['Name']+')',name['Country'])    \n",
    "    \n",
    "df=pd.DataFrame(list)\n",
    "df.to_csv('10_ODI_Batsmen_Teams.csv',index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee85b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb81d43e",
   "metadata": {},
   "source": [
    "c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e5459c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (Trent Boult) \n",
      "2 (Josh Hazlewood) AUS\n",
      "3 (Mujeeb Ur Rahman) AFG\n",
      "4 (Jasprit Bumrah) IND\n",
      "5 (Shaheen Afridi) PAK\n",
      "6 (Mohammad Nabi) AFG\n",
      "7 (Mehedi Hasan) BAN\n",
      "8 (Rashid Khan) AFG\n",
      "9 (Matt Henry) NZ\n",
      "10 (Mustafizur Rahman) BAN\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url='https://www.icc-cricket.com/rankings/mens/player-rankings/odi'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "# Find batting Section\n",
    "records=soup.find(\"div\", {\"data-cricket-role\" : \"bowling\"})\n",
    "\n",
    "# For ranking 1st only\n",
    "ranking = records.find_all(\"span\",{\"class\" : \"rankings-block__pos-number\"})\n",
    "names = records.find_all(\"div\",{\"class\" : \"rankings-block__banner--name\"})\n",
    "country = records.find_all(\"div\",{\"class\" : \"flag-15\"})\n",
    "\n",
    "list=[]\n",
    "\n",
    "for index in range(0,len(names)): \n",
    "    data = {\n",
    "        \"Ranking\":ranking[0].get_text().strip(),\n",
    "        \"Name\":names[0].get_text().strip(),\n",
    "        \"Country\":country[0].get_text().strip()\n",
    "    }\n",
    "    list.append(data)\n",
    "\n",
    "\n",
    "#For ranking from 2nd to 10th\n",
    "url='https://www.icc-cricket.com/rankings/mens/player-rankings/odi'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "# Find bowling Section\n",
    "records=soup.find(\"div\", {\"data-cricket-role\" : \"bowling\"})\n",
    "\n",
    "rankings = records.find_all(\"span\",{\"class\" : \"rankings-table__pos-number\"})\n",
    "names = records.find_all(\"td\",{\"class\" : \"table-body__cell name\"})\n",
    "countries = records.find_all(\"span\",{\"class\" : \"table-body__logo-text\"})\n",
    "\n",
    "for index in range(0,len(names)): \n",
    "    data = {\n",
    "        \"Ranking\":rankings[index].get_text().strip(),\n",
    "        \"Name\":names[index].get_text().strip(),\n",
    "        \"Country\":countries[index].get_text().strip()    \n",
    "    }\n",
    "    list.append(data)\n",
    "    \n",
    "for name in list:\n",
    "    print(name['Ranking'],'('+name['Name']+')',name['Country'])    \n",
    "    \n",
    "df=pd.DataFrame(list)\n",
    "df.to_csv('10_ODI_Bowlers_Teams.csv',index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4681a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f138929b",
   "metadata": {},
   "source": [
    "6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "    a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "    b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "    c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e7fea",
   "metadata": {},
   "source": [
    " a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2cc03e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia (29) 4,837 167\n",
      "England (33) 4,046 123\n",
      "South Africa (35) 4,157 119\n",
      "India (32) 3,219 101\n",
      "New Zealand (31) 3,019 97\n",
      "West Indies (30) 2,768 92\n",
      "Bangladesh (12) 930 78\n",
      "Pakistan (30) 1,962 65\n",
      "Ireland (9) 405 45\n",
      "Sri Lanka (11) 495 45\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url='https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "teams=soup.select('td.rankings-block__banner--team-name span.u-hide-phablet')\n",
    "matches=soup.select('tr.rankings-block__banner td.rankings-block__banner--matches')\n",
    "points=soup.select('td.rankings-block__banner--points')\n",
    "ratings=soup.select('td.rankings-block__banner--rating.u-text-right')\n",
    "\n",
    "list=[]\n",
    "\n",
    "for index in range(0,len(teams)): \n",
    "    data = {\n",
    "        \"Team\":teams[index].get_text(),\n",
    "        \"Match\":matches[index].get_text(),\n",
    "        \"Points\":points[index].get_text(),\n",
    "        \"Rating\":ratings[index].get_text().strip()\n",
    "    }\n",
    "    list.append(data)\n",
    "\n",
    "url='https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "teams=soup.select('td.table-body__cell.rankings-table__team span.u-hide-phablet')\n",
    "matches=soup.select('td.table-body__cell.u-center-text')[::2]\n",
    "points=soup.select('td.table-body__cell.u-center-text')[1::2]\n",
    "ratings=soup.select('td.table-body__cell.u-text-right.rating')\n",
    "\n",
    "\n",
    "for index in range(0,9): \n",
    "    data = {\n",
    "        \"Team\":teams[index].get_text(),\n",
    "        \"Match\":matches[index].get_text(),\n",
    "        \"Points\":points[index].get_text(),\n",
    "        \"Rating\":ratings[index].get_text()\n",
    "    }\n",
    "    list.append(data)\n",
    "    \n",
    "    \n",
    "for name in list:\n",
    "    print(name['Team'],'('+name['Match']+')',name['Points'],name['Rating'])\n",
    "\n",
    "df=pd.DataFrame(list)\n",
    "df.to_csv('10_ODI_Women_Teams.csv',index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b2aca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db3d68e9",
   "metadata": {},
   "source": [
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "754c8b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (Alyssa Healy) AUS\n",
      "2 (Beth Mooney) AUS\n",
      "3 (Natalie Sciver) ENG\n",
      "4 (Laura Wolvaardt) SA\n",
      "5 (Meg Lanning) AUS\n",
      "6 (Rachael Haynes) AUS\n",
      "7 (Amy Satterthwaite) NZ\n",
      "8 (Tammy Beaumont) ENG\n",
      "9 (Chamari Athapaththu) SL\n",
      "10 (Smriti Mandhana) IND\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url='https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "\n",
    "# Find batting Section\n",
    "records=soup.find(\"div\", {\"data-cricket-role\" : \"batting\"})\n",
    "\n",
    "# For ranking 1st only\n",
    "ranking = records.find_all(\"span\",{\"class\" : \"rankings-block__pos-number\"})\n",
    "names = records.find_all(\"div\",{\"class\" : \"rankings-block__banner--name-large\"})\n",
    "country = records.find_all(\"div\",{\"class\" : \"rankings-block__banner--nationality\"})\n",
    "\n",
    "list=[]\n",
    "\n",
    "for index in range(0,len(names)): \n",
    "    data = {\n",
    "        \"Ranking\":ranking[0].get_text().strip(),\n",
    "        \"Name\":names[0].get_text().strip(),\n",
    "        \"Country\":country[0].get_text().strip()\n",
    "    }\n",
    "    list.append(data)\n",
    "\n",
    "#For ranking from 2nd to 10th\n",
    "url='https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "# Find batting Section\n",
    "records=soup.find(\"div\", {\"data-cricket-role\" : \"batting\"})\n",
    "\n",
    "ranking = records.find_all(\"span\",{\"class\" : \"rankings-table__pos-number\"})\n",
    "names = records.find_all(\"td\",{\"class\" : \"table-body__cell rankings-table__name name\"})\n",
    "country = records.find_all(\"span\",{\"class\" : \"table-body__logo-text\"})\n",
    "\n",
    "for index in range(0,9): \n",
    "    data = {\n",
    "        \"Ranking\":ranking[index].get_text().strip(),\n",
    "        \"Name\":names[index].get_text().strip(),          \n",
    "        \"Country\":country[index].get_text().strip()    \n",
    "    }\n",
    "    list.append(data)\n",
    "    \n",
    "for name in list:\n",
    "    print(name['Ranking'],'('+name['Name']+')',name['Country'])    \n",
    "    \n",
    "df=pd.DataFrame(list)\n",
    "df.to_csv('10_Womens_ODI_Batting.csv',index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80a2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43be845f",
   "metadata": {},
   "source": [
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f355e510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (Natalie Sciver) ENG\n",
      "2 (Ellyse Perry) AUS\n",
      "3 (Marizanne Kapp) SA\n",
      "4 (Hayley Matthews) WI\n",
      "5 (Amelia Kerr) NZ\n",
      "6 (Ashleigh Gardner) AUS\n",
      "7 (Deepti Sharma) IND\n",
      "8 (Jess Jonassen) AUS\n",
      "9 (Katherine Brunt) ENG\n",
      "10 (Stafanie Taylor) WI\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url='https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "# Find AllRounder Section\n",
    "records=soup.find(\"div\", {\"data-cricket-role\" : \"all_round\"})\n",
    "\n",
    "# For ranking 1st only\n",
    "ranking = records.find_all(\"span\",{\"class\" : \"rankings-block__pos-number\"})\n",
    "names = records.find_all(\"div\",{\"class\" : \"rankings-block__banner--name-large\"})\n",
    "country = records.find_all(\"div\",{\"class\" : \"rankings-block__banner--nationality\"})\n",
    "ratings = records.find_all(\"td\",{\"class\" : \"table-body__cell rating\"})\n",
    "\n",
    "list=[]\n",
    "\n",
    "for index in range(0,len(names)): \n",
    "    data = {\n",
    "        \"Ranking\":ranking[0].get_text().strip(),\n",
    "        \"Name\":names[0].get_text().strip(),\n",
    "        \"Country\":country[0].get_text().strip(),\n",
    "        \"Rating\":ratings[0].get_text().strip()\n",
    "    }\n",
    "    list.append(data)\n",
    "    \n",
    "#For ranking from 2nd to 10th\n",
    "url='https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "# Find AllRounder Section\n",
    "records=soup.find(\"div\", {\"data-cricket-role\" : \"all_round\"})\n",
    "\n",
    "ranking = records.find_all(\"span\",{\"class\" : \"rankings-table__pos-number\"})\n",
    "names = records.find_all(\"td\",{\"class\" : \"table-body__cell rankings-table__name name\"})\n",
    "country = records.find_all(\"span\",{\"class\" : \"table-body__logo-text\"})\n",
    "ratings = records.find_all(\"td\",{\"class\" : \"table-body__cell rating\"})\n",
    "\n",
    "for index in range(0,9): \n",
    "    data = {\n",
    "        \"Ranking\":ranking[index].get_text().strip(),\n",
    "        \"Name\":names[index].get_text().strip(),          \n",
    "        \"Country\":country[index].get_text().strip(),\n",
    "        \"Rating\":ratings[0].get_text().strip()\n",
    "    }\n",
    "    list.append(data)\n",
    "    \n",
    "for name in list:\n",
    "    print(name['Ranking'],'('+name['Name']+')',name['Country'])    \n",
    "    \n",
    "df=pd.DataFrame(list)\n",
    "df.to_csv('10_Womens_ODI_AllRounder.csv',index=False) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeba8edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62db2564",
   "metadata": {},
   "source": [
    "7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec31945b",
   "metadata": {},
   "source": [
    "i) Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3095ce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukraine on edge as shellfire resounds around Zaporizhzhia nuclear plant \n",
      "\n",
      "U.S. companies are reshoring at a rapid pace. Here's how to play the trend\n",
      "NASA set to launch the Artemis 1 mission on its most powerful rocket – here's what you should know \n",
      "U.S. warships transit Taiwan Strait, first since Pelosi visit \n",
      "ECB policymakers make the case for a big rate hike \n",
      "\n",
      "Testing June’s stock market low isn’t a foregone conclusion\n",
      "A key leader for Meta's metaverse software is leaving the company\n",
      "Pakistan floods force tens of thousands from homes overnight\n",
      "Russia and Ukraine accuse each other of shelling around Europe's largest nuclear power plant\n",
      "Zelenskyy says world narrowly avoided nuclear catastrophe; Putin to boost size of Russian military\n",
      "Got a text from a wrong number? It could be 'pig butchering,' a crypto scam costing victims millions\n",
      "Climate change could bring back wind as the future power source for ocean cargo ships\n",
      "DOJ reveals redacted affidavit justifying Trump Mar-a-Lago raid\n",
      "6 therapist-recommended books to read if you want to learn more about your attachment style\n",
      "\n",
      "Powell's speech delivers tough lesson to markets: 'Don't fight the Fed' \n",
      "Russia divestment promises by U.S. states go largely unfulfilled \n",
      "Google employees frustrated after office Covid outbreaks, some call to modify vaccine policy\n",
      "'Something is seriously wrong': Postal workers latest to strike as UK cost chaos continues\n"
     ]
    }
   ],
   "source": [
    "url='https://www.cnbc.com/world/?region=world'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "records=soup.select('div.RiverPlus-riverPlusContainer div.RiverPlusCard-cardLeft div.RiverHeadline-headline a')\n",
    "\n",
    "for index in range(0,len(records)): \n",
    "    print(records[index].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9110d178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "630d5bfb",
   "metadata": {},
   "source": [
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b8423ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.cnbc.com/2022/08/28/ukraine-on-edge-as-shellfire-resounds-around-zaporizhzhia-nuclear-plant.html\n",
      "/pro/\n",
      "https://www.cnbc.com/2022/08/27/us-companies-are-reshoring-at-a-rapid-pace-heres-how-to-play-the-trend.html\n",
      "https://www.cnbc.com/2022/08/27/nasas-artemis-1-mission-what-you-should-know-about-sls-orion.html\n",
      "https://www.cnbc.com/2022/08/28/us-warships-transit-taiwan-strait-first-since-pelosi-visit.html\n",
      "https://www.cnbc.com/2022/08/28/ecb-policymakers-make-the-case-for-a-big-rate-hike-.html\n",
      "/pro/\n",
      "https://www.cnbc.com/2022/08/27/testing-junes-stock-market-low-isnt-a-foregone-conclusion.html\n",
      "https://www.cnbc.com/2022/08/26/meta-horizon-vp-vivek-sharma-leaving.html\n",
      "https://www.cnbc.com/2022/08/28/pakistan-floods-force-tens-of-thousands-from-homes-overnight.html\n",
      "https://www.cnbc.com/2022/08/27/russia-and-ukraine-accuse-each-other-of-shelling-around-zaporizhzhia-nuclear-plant.html\n",
      "https://www.cnbc.com/2022/08/26/russia-ukraine-live-updates.html\n",
      "https://www.cnbc.com/2022/08/25/pig-butchering-crypto-scam-costing-investors-millions.html\n",
      "https://www.cnbc.com/2022/08/27/how-ocean-shipping-goes-green-from-wind-power-to-liquid-hydrogen.html\n",
      "https://www.cnbc.com/2022/08/26/doj-reveals-redacted-affidavit-justifying-trump-mar-a-lago-raid.html\n",
      "https://www.cnbc.com/2022/08/27/6-books-to-read-if-you-want-to-learn-more-about-your-attachment-style.html\n",
      "/pro/\n",
      "https://www.cnbc.com/2022/08/26/powells-speech-delivers-tough-lesson-to-markets-dont-fight-the-fed-.html\n",
      "https://www.cnbc.com/2022/08/27/russia-divestment-promises-by-us-states-go-largely-unfulfilled-.html\n",
      "https://www.cnbc.com/2022/08/26/google-employees-frustrated-after-office-covid-outbreaks.html\n",
      "https://www.cnbc.com/2022/08/26/royal-mail-postal-workers-latest-to-strike-as-uk-cost-chaos-continues.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    " \n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "reqs = requests.get(url)\n",
    "soup = BeautifulSoup(reqs.text, 'lxml')\n",
    "results =  soup.select('div.RiverPlus-riverPlusContainer div div div.RiverPlusCard-cardLeft div.RiverHeadline-headline a')\n",
    "#print(results)\n",
    "\n",
    "urls = []\n",
    "for link in results:\n",
    "    print(link.get('href'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff106c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73587736",
   "metadata": {},
   "source": [
    "8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days. \n",
    "https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details :\n",
    "i) Paper Title \n",
    "ii) Authors\n",
    "iii) Published Date \n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10acb89c",
   "metadata": {},
   "source": [
    "i) Paper Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c4687b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward is enough\n",
      "Making sense of raw input\n",
      "Law and logic: A review from an argumentation perspective\n",
      "Creativity and artificial intelligence\n",
      "Artificial cognition for social human–robot interaction: An implementation\n",
      "Explanation in artificial intelligence: Insights from the social sciences\n",
      "Making sense of sensory input\n",
      "Conflict-based search for optimal multi-agent pathfinding\n",
      "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning\n",
      "The Hanabi challenge: A new frontier for AI research\n",
      "Evaluating XAI: A comparison of rule-based and example-based explanations\n",
      "Argumentation in artificial intelligence\n",
      "Algorithms for computing strategies in two-player simultaneous move games\n",
      "Multiple object tracking: A literature review\n",
      "Selection of relevant features and examples in machine learning\n",
      "A survey of inverse reinforcement learning: Challenges, methods and progress\n",
      "Explaining individual predictions when features are dependent: More accurate approximations to Shapley values\n",
      "A review of possible effects of cognitive biases on interpretation of rule-based machine learning models\n",
      "Integrating social power into the decision-making of cognitive agents\n",
      "“That's (not) the output I expected!” On the role of end user expectations in creating explanations of AI systems\n",
      "Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies\n",
      "Algorithm runtime prediction: Methods & evaluation\n",
      "Wrappers for feature subset selection\n",
      "Commonsense visual sensemaking for autonomous driving – On generalised neurosymbolic online abduction integrating vision and semantics\n",
      "Quantum computation, quantum theory and AI\n"
     ]
    }
   ],
   "source": [
    "url='https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "\n",
    "title=soup.select('ul li article a h2')\n",
    "\n",
    "for index in range(0,len(title)): \n",
    "    print(title[index].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11474f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51746899",
   "metadata": {},
   "source": [
    "ii) Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e144f74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors\n",
      "\n",
      "Submit your Paper\n",
      "Check Submitted Paper\n",
      "Researcher Academy\n",
      "Rights and Permissions\n",
      "Elsevier Author Services\n",
      "Support Center\n",
      "Track Accepted Paper\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    " \n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "reqs = requests.get(url)\n",
    "soup = BeautifulSoup(reqs.text, 'lxml')\n",
    "\n",
    "section = soup.find_all(\"div\",{\"class\" : \"sc-1nwsl0e-4\"})\n",
    "authorSection = section[1]\n",
    "\n",
    "authors = authorSection.find_all(\"span\")\n",
    "\n",
    "for index in range(0,len(authors)):\n",
    "    print(authors[index].get_text())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c21ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fec6fe7f",
   "metadata": {},
   "source": [
    "iii) Published Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "708f4235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward is enough - October 2021\n",
      "Making sense of raw input - October 2021\n",
      "Law and logic: A review from an argumentation perspective - October 2015\n",
      "Creativity and artificial intelligence - August 1998\n",
      "Artificial cognition for social human–robot interaction: An implementation - June 2017\n",
      "Explanation in artificial intelligence: Insights from the social sciences - February 2019\n",
      "Making sense of sensory input - April 2021\n",
      "Conflict-based search for optimal multi-agent pathfinding - February 2015\n",
      "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning - August 1999\n",
      "The Hanabi challenge: A new frontier for AI research - March 2020\n",
      "Evaluating XAI: A comparison of rule-based and example-based explanations - February 2021\n",
      "Argumentation in artificial intelligence - October 2007\n",
      "Algorithms for computing strategies in two-player simultaneous move games - August 2016\n",
      "Multiple object tracking: A literature review - April 2021\n",
      "Selection of relevant features and examples in machine learning - December 1997\n",
      "A survey of inverse reinforcement learning: Challenges, methods and progress - August 2021\n",
      "Explaining individual predictions when features are dependent: More accurate approximations to Shapley values - September 2021\n",
      "A review of possible effects of cognitive biases on interpretation of rule-based machine learning models - June 2021\n",
      "Integrating social power into the decision-making of cognitive agents - December 2016\n",
      "“That's (not) the output I expected!” On the role of end user expectations in creating explanations of AI systems - September 2021\n",
      "Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies - May 2021\n",
      "Algorithm runtime prediction: Methods & evaluation - January 2014\n",
      "Wrappers for feature subset selection - December 1997\n",
      "Commonsense visual sensemaking for autonomous driving – On generalised neurosymbolic online abduction integrating vision and semantics - October 2021\n",
      "Quantum computation, quantum theory and AI - February 2010\n"
     ]
    }
   ],
   "source": [
    "url='https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "\n",
    "title=soup.select('ul li article a h2')\n",
    "\n",
    "date=soup.select('ul li article p span span')\n",
    "\n",
    "list=[]\n",
    "\n",
    "for index in range(0,len(title)): \n",
    "    data={\n",
    "        \"Title\":title[index].get_text(),\n",
    "        \"Date\":date[index].get_text()\n",
    "    }\n",
    "    list.append(data)\n",
    "\n",
    "        \n",
    "for name in list:\n",
    "    print(name['Title'],'-',name['Date'])\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ddb947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaf9808d",
   "metadata": {},
   "source": [
    "iv) Paper URL \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7c023d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sciencedirect.com/science/article/pii/S0004370221000862\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000722\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370215000910\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370298000551\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370216300790\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370218305988\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370220301855\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370214001386\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370299000521\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370219300116\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370220301533\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370207000793\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370216300285\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370220301958\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370297000635\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000515\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000539\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000096\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370216300868\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000588\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000102\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370213001082\n",
      "https://www.sciencedirect.com/science/article/pii/S000437029700043X\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000734\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370209001398\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    " \n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "reqs = requests.get(url)\n",
    "soup = BeautifulSoup(reqs.text, 'lxml')\n",
    "results =  soup.select('div.sc-orwwe2-3.gPmFkB ul.sc-9zxyh7-0.ffmPq li.sc-9zxyh7-1.sc-9zxyh7-2.exAXfr.jQmQZp a')\n",
    "#print(results)\n",
    "\n",
    "urls = []\n",
    "for link in results:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a5d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "449b7eba",
   "metadata": {},
   "source": [
    "9) Write a python program to scrape mentioned details from dineout.co.in :\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location \n",
    "iv) Ratings\n",
    "v) Image URL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1cdd19",
   "metadata": {},
   "source": [
    "i) Restaurant name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "10abeed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tyagi's EateryAlkapuri, Central Vadodara\n",
      "The MorselVadiwadi, Central Vadodara\n",
      "Charmant - The Heritage CaféAlkapuri, Central Vadodara\n",
      "The Glass HouseAlkapuri, Central Vadodara\n"
     ]
    }
   ],
   "source": [
    "url='https://www.dineout.co.in/vadodara'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "records=soup.select('div._3ZUwv div.GxiC8 div._1JdPN.PGkDX div._9FNkO')\n",
    "\n",
    "\n",
    "for index in range(0,len(records)): \n",
    "    print(records[index].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4423ca4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94bbd1ca",
   "metadata": {},
   "source": [
    "ii) Cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "74ab857d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "North Indian\n",
      "Chinese\n",
      "Italian\n",
      "Fast Food\n",
      "Continental\n",
      "Multi Cuisine\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    " \n",
    "url = 'https://www.dineout.co.in/vadodara'\n",
    "reqs = requests.get(url)\n",
    "soup = BeautifulSoup(reqs.text, 'lxml')\n",
    "\n",
    "section = soup.find(\"div\",{\"class\" : \"_3p5zZ akYDu\"})\n",
    "#print(section)\n",
    "#cuisinesection=section[1]\n",
    "Cuisine = section.find_all(\"li\")\n",
    "\n",
    "cuisinesection= location[11:17]\n",
    "\n",
    "\n",
    "#print(Cuisine)\n",
    "for index in range(0,len(cuisinesection)):\n",
    "    print(cuisinesection[index].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e511ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b64a810c",
   "metadata": {},
   "source": [
    "iii) Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "e86252a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alkapuri\n",
      "Gotri\n",
      "Fatehgunj\n",
      "Sayajiganj\n",
      "Akota\n",
      "Old Padra Road\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    " \n",
    "url = 'https://www.dineout.co.in/vadodara'\n",
    "reqs = requests.get(url)\n",
    "soup = BeautifulSoup(reqs.text, 'lxml')\n",
    "\n",
    "section = soup.find(\"div\",{\"class\" : \"_3p5zZ akYDu\"})\n",
    "#print(section)\n",
    "\n",
    "location = section.find_all(\"a\")\n",
    "\n",
    "locationsection= location[20:26]\n",
    "\n",
    "#print(locationsection)\n",
    "for index in range(0,len(locationsection)):\n",
    "    print(locationsection[index].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1111ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f016f38f",
   "metadata": {},
   "source": [
    "iv) Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f4000305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tyagi's EateryAlkapuri, Central Vadodara - 4.3\n",
      "The MorselVadiwadi, Central Vadodara - 4.4\n",
      "Charmant - The Heritage CaféAlkapuri, Central Vadodara - 4.2\n",
      "The Glass HouseAlkapuri, Central Vadodara - 4.0\n"
     ]
    }
   ],
   "source": [
    "url='https://www.dineout.co.in/vadodara'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "records=soup.select('div._3ZUwv div.GxiC8 div._1JdPN.PGkDX div._9FNkO')\n",
    "ratings=soup.select('div._3ZUwv div.GxiC8 div._1JdPN.PGkDX div.kGUdK._1oTbl')\n",
    "\n",
    "list=[]\n",
    "\n",
    "for index in range(0,len(records)): \n",
    "    data = {\n",
    "        \"Records\":records[index].get_text(),\n",
    "        \"Ratings\":ratings[index].get_text()\n",
    "    }\n",
    "    list.append(data)\n",
    "    \n",
    "for name in list:\n",
    "    print(name['Records'],'-',name['Ratings'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b673693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0c943c6",
   "metadata": {},
   "source": [
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b472ab0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 'https://im1.dineout.co.in/images/uploads/misc/2020/Sep/9/artboard.png?tr=tr:n-large', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/9/k/x/p94405-162729935160fe9e17b295a.jpg?tr=tr:n-large', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/1/p/a/p103007-164198903161dec3a7ea27f.jpg?tr=tr:n-large', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/6/y/c/p65318-1644996190620ca65ed20c6.jpg?tr=tr:n-large', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/6/v/l/p65290-15685243215d7dc821e5055.jpg?tr=tr:n-large', None]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#import urllib2 \n",
    " \n",
    "url = 'https://www.dineout.co.in/vadodara'\n",
    "reqs = requests.get(url)\n",
    "soup = BeautifulSoup(reqs.text, 'lxml')\n",
    "results =  soup.select('div ul li a div div')\n",
    "#print(results)\n",
    "\n",
    "images = []\n",
    "for img in soup.findAll('img'):\n",
    "    images.append(img.get('data-src'))\n",
    "\n",
    "print(images)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51f7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6314d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "10) Write a python program to scrape the details of top publications from Google Scholar from \n",
    "https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "i) Rank \n",
    "ii) Publication\n",
    "iii) h5-index\n",
    " iv) h5-median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b6a57",
   "metadata": {},
   "source": [
    "i) Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "b6cd6583",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "2.\n",
      "3.\n",
      "4.\n",
      "5.\n",
      "6.\n",
      "7.\n",
      "8.\n",
      "9.\n",
      "10.\n",
      "11.\n",
      "12.\n",
      "13.\n",
      "14.\n",
      "15.\n",
      "16.\n",
      "17.\n",
      "18.\n",
      "19.\n",
      "20.\n",
      "21.\n",
      "22.\n",
      "23.\n",
      "24.\n",
      "25.\n",
      "26.\n",
      "27.\n",
      "28.\n",
      "29.\n",
      "30.\n",
      "31.\n",
      "32.\n",
      "33.\n",
      "34.\n",
      "35.\n",
      "36.\n",
      "37.\n",
      "38.\n",
      "39.\n",
      "40.\n",
      "41.\n",
      "42.\n",
      "43.\n",
      "44.\n",
      "45.\n",
      "46.\n",
      "47.\n",
      "48.\n",
      "49.\n",
      "50.\n",
      "51.\n",
      "52.\n",
      "53.\n",
      "54.\n",
      "55.\n",
      "56.\n",
      "57.\n",
      "58.\n",
      "59.\n",
      "60.\n",
      "61.\n",
      "62.\n",
      "63.\n",
      "64.\n",
      "65.\n",
      "66.\n",
      "67.\n",
      "68.\n",
      "69.\n",
      "70.\n",
      "71.\n",
      "72.\n",
      "73.\n",
      "74.\n",
      "75.\n",
      "76.\n",
      "77.\n",
      "78.\n",
      "79.\n",
      "80.\n",
      "81.\n",
      "82.\n",
      "83.\n",
      "84.\n",
      "85.\n",
      "86.\n",
      "87.\n",
      "88.\n",
      "89.\n",
      "90.\n",
      "91.\n",
      "92.\n",
      "93.\n",
      "94.\n",
      "95.\n",
      "96.\n",
      "97.\n",
      "98.\n",
      "99.\n",
      "100.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url='https://scholar.google.com/citations?view_op=top_venues&hl=en'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "table1 = soup.find('table', id='gsc_mvt_table')\n",
    "rank = table1.find_all(\"td\",{\"class\" : \"gsc_mvt_p\"})\n",
    "\n",
    "list=[]\n",
    "\n",
    "for index in range(0,len(rank)): \n",
    "    data = {\n",
    "        \"Rank\":rank[index].get_text()      \n",
    "    }\n",
    "    list.append(data)\n",
    "    \n",
    "for name in list:\n",
    "    print(name['Rank']) \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeaccd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9023872",
   "metadata": {},
   "source": [
    "ii) Publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "9f98d30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nature\n",
      "The New England Journal of Medicine\n",
      "Science\n",
      "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n",
      "The Lancet\n",
      "Advanced Materials\n",
      "Nature Communications\n",
      "Cell\n",
      "International Conference on Learning Representations\n",
      "Neural Information Processing Systems\n",
      "JAMA\n",
      "Chemical Reviews\n",
      "Proceedings of the National Academy of Sciences\n",
      "Angewandte Chemie\n",
      "Chemical Society Reviews\n",
      "Journal of the American Chemical Society\n",
      "IEEE/CVF International Conference on Computer Vision\n",
      "Nucleic Acids Research\n",
      "International Conference on Machine Learning\n",
      "Nature Medicine\n",
      "Renewable and Sustainable Energy Reviews\n",
      "Science of The Total Environment\n",
      "Advanced Energy Materials\n",
      "Journal of Clinical Oncology\n",
      "ACS Nano\n",
      "Journal of Cleaner Production\n",
      "Advanced Functional Materials\n",
      "Physical Review Letters\n",
      "Scientific Reports\n",
      "The Lancet Oncology\n",
      "Energy & Environmental Science\n",
      "IEEE Access\n",
      "PLoS ONE\n",
      "Science Advances\n",
      "Journal of the American College of Cardiology\n",
      "Applied Catalysis B: Environmental\n",
      "Nature Genetics\n",
      "BMJ\n",
      "Circulation\n",
      "European Conference on Computer Vision\n",
      "International Journal of Molecular Sciences\n",
      "Nature Materials\n",
      "Chemical engineering journal\n",
      "AAAI Conference on Artificial Intelligence\n",
      "Journal of Materials Chemistry A\n",
      "ACS Applied Materials & Interfaces\n",
      "Nature Biotechnology\n",
      "The Lancet Infectious Diseases\n",
      "Frontiers in Immunology\n",
      "Applied Energy\n",
      "Nano Energy\n",
      "Nature Energy\n",
      "Meeting of the Association for Computational Linguistics (ACL)\n",
      "The Astrophysical Journal\n",
      "Gastroenterology\n",
      "Nature Methods\n",
      "IEEE Transactions on Pattern Analysis and Machine Intelligence\n",
      "Cochrane Database of Systematic Reviews\n",
      "Blood\n",
      "Neuron\n",
      "Nano Letters\n",
      "Morbidity and Mortality Weekly Report\n",
      "European Heart Journal\n",
      "Nature Nanotechnology\n",
      "ACS Catalysis\n",
      "Nature Neuroscience\n",
      "American Economic Review\n",
      "Journal of High Energy Physics\n",
      "IEEE Communications Surveys & Tutorials\n",
      "Annals of Oncology\n",
      "Nutrients\n",
      "Accounts of Chemical Research\n",
      "Immunity\n",
      "Environmental Science & Technology\n",
      "Nature Reviews. Molecular Cell Biology\n",
      "Gut\n",
      "Physical Review D\n",
      "ACS Energy Letters\n",
      "Monthly Notices of the Royal Astronomical Society\n",
      "Conference on Empirical Methods in Natural Language Processing (EMNLP)\n",
      "Clinical Infectious Diseases\n",
      "Cell Metabolism\n",
      "Nature Reviews Immunology\n",
      "Joule\n",
      "Nature Photonics\n",
      "International Journal of Environmental Research and Public Health\n",
      "Environmental Pollution\n",
      "Computers in Human Behavior\n",
      "Frontiers in Microbiology\n",
      "Nature Physics\n",
      "Small\n",
      "Cell Reports\n",
      "Molecular Cell\n",
      "Clinical Cancer Research\n",
      "Bioresource Technology\n",
      "Journal of Business Research\n",
      "Molecular Cancer\n",
      "Sensors\n",
      "Nature Climate Change\n",
      "IEEE Internet of Things Journal\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url='https://scholar.google.com/citations?view_op=top_venues&hl=en'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "table1 = soup.find('table', id='gsc_mvt_table')\n",
    "publication = table1.find_all(\"td\",{\"class\" : \"gsc_mvt_t\"})\n",
    "\n",
    "list=[]\n",
    "\n",
    "for index in range(0,len(publication)): \n",
    "    data = {\n",
    "        \"Publication\":publication[index].get_text()      \n",
    "    }\n",
    "    list.append(data)\n",
    "    \n",
    "for name in list:\n",
    "    print(name['Publication']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144acec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f305a590",
   "metadata": {},
   "source": [
    "iii) h5-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "aee920cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444\n",
      "432\n",
      "401\n",
      "389\n",
      "354\n",
      "312\n",
      "307\n",
      "300\n",
      "286\n",
      "278\n",
      "267\n",
      "265\n",
      "256\n",
      "245\n",
      "244\n",
      "242\n",
      "239\n",
      "238\n",
      "237\n",
      "235\n",
      "227\n",
      "225\n",
      "220\n",
      "213\n",
      "211\n",
      "211\n",
      "210\n",
      "207\n",
      "206\n",
      "202\n",
      "202\n",
      "200\n",
      "198\n",
      "197\n",
      "195\n",
      "192\n",
      "191\n",
      "190\n",
      "189\n",
      "186\n",
      "183\n",
      "181\n",
      "181\n",
      "180\n",
      "178\n",
      "177\n",
      "175\n",
      "173\n",
      "173\n",
      "173\n",
      "172\n",
      "170\n",
      "169\n",
      "167\n",
      "166\n",
      "165\n",
      "165\n",
      "165\n",
      "165\n",
      "164\n",
      "164\n",
      "163\n",
      "163\n",
      "163\n",
      "163\n",
      "162\n",
      "160\n",
      "160\n",
      "159\n",
      "159\n",
      "159\n",
      "159\n",
      "158\n",
      "158\n",
      "155\n",
      "155\n",
      "155\n",
      "155\n",
      "155\n",
      "154\n",
      "153\n",
      "153\n",
      "152\n",
      "152\n",
      "152\n",
      "152\n",
      "152\n",
      "152\n",
      "151\n",
      "151\n",
      "150\n",
      "149\n",
      "149\n",
      "146\n",
      "146\n",
      "145\n",
      "145\n",
      "145\n",
      "144\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url='https://scholar.google.com/citations?view_op=top_venues&hl=en'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "table1 = soup.find('table', id='gsc_mvt_table')\n",
    "h5 = table1.find_all(\"a\",{\"class\" : \"gs_ibl gsc_mp_anchor\"})\n",
    "\n",
    "list=[]\n",
    "\n",
    "for index in range(0,len(h5)): \n",
    "    data = {\n",
    "        \"H5\":h5[index].get_text()      \n",
    "    }\n",
    "    list.append(data)\n",
    "    \n",
    "for name in list:\n",
    "    print(name['H5']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a0f0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0b15fff",
   "metadata": {},
   "source": [
    "iv) h5-median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "4a63a05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "667\n",
      "780\n",
      "614\n",
      "627\n",
      "635\n",
      "418\n",
      "428\n",
      "505\n",
      "533\n",
      "436\n",
      "425\n",
      "444\n",
      "364\n",
      "332\n",
      "386\n",
      "344\n",
      "415\n",
      "550\n",
      "421\n",
      "389\n",
      "324\n",
      "311\n",
      "300\n",
      "315\n",
      "277\n",
      "273\n",
      "280\n",
      "294\n",
      "274\n",
      "329\n",
      "290\n",
      "303\n",
      "278\n",
      "294\n",
      "276\n",
      "246\n",
      "297\n",
      "307\n",
      "301\n",
      "321\n",
      "253\n",
      "265\n",
      "224\n",
      "296\n",
      "220\n",
      "223\n",
      "315\n",
      "296\n",
      "228\n",
      "217\n",
      "232\n",
      "314\n",
      "304\n",
      "234\n",
      "254\n",
      "296\n",
      "293\n",
      "243\n",
      "229\n",
      "231\n",
      "207\n",
      "302\n",
      "265\n",
      "264\n",
      "220\n",
      "248\n",
      "263\n",
      "220\n",
      "304\n",
      "243\n",
      "214\n",
      "211\n",
      "242\n",
      "214\n",
      "340\n",
      "235\n",
      "217\n",
      "212\n",
      "194\n",
      "249\n",
      "278\n",
      "211\n",
      "292\n",
      "233\n",
      "228\n",
      "225\n",
      "222\n",
      "214\n",
      "225\n",
      "222\n",
      "196\n",
      "205\n",
      "202\n",
      "201\n",
      "190\n",
      "233\n",
      "209\n",
      "201\n",
      "228\n",
      "212\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "url='https://scholar.google.com/citations?view_op=top_venues&hl=en'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "table1 = soup.find('table', id='gsc_mvt_table')\n",
    "median = table1.find_all(\"span\",{\"class\" : \"gs_ibl gsc_mp_anchor\"})\n",
    "\n",
    "list=[]\n",
    "\n",
    "for index in range(0,len(median)): \n",
    "    data = {\n",
    "        \"Median\":median[index].get_text()      \n",
    "    }\n",
    "    list.append(data)\n",
    "    \n",
    "for name in list:\n",
    "    print(name['Median']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb61fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
